"""
Servi√ßo Completo de IA para Suporte Emocional - Vers√£o Modular
Inclui: An√°lise de Sentimentos, Gera√ß√£o de Respostas, RAG e Sistema de Prompts

"""

import json
import os
import logging
import random
from datetime import datetime, UTC
from typing import Dict, List, Optional
from sqlalchemy import text

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# === DEPEND√äNCIAS OPCIONAIS ===
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


try:
    from transformers import pipeline
    BERT_AVAILABLE = True
except ImportError:
    BERT_AVAILABLE = False


# === IMPORTAR SISTEMA DE PROMPTS ===
from .ai_prompt import AIPromptManager

# === IMPORTAR SISTEMA DE LOGGING DE TREINAMENTO ===
from .training_usage_logger import training_usage_logger, log_ai_response_with_training_check

# === IMPORTAR SISTEMAS AVAN√áADOS ===
from .advanced_rag_service import advanced_rag_service
from .advanced_prompt_engineer import advanced_prompt_engineer, PromptContext, RiskLevel, PromptType
from .finetuning_preparator import finetuning_preparator

# Configurar logging
logger = logging.getLogger(__name__)


class SimpleRAG:
    """
    Sistema RAG (Retrieval-Augmented Generation) Profissional
    
    Funcionalidades:
    - Busca conversas similares bem-sucedidas no banco
    - Extra√ß√£o inteligente de palavras-chave
    - Ranking por relev√¢ncia com m√∫ltiplos fatores
    - Cache para otimiza√ß√£o de performance
    - N√£o requer banco vetorial complexo
    """
    
    def __init__(self):
        """Inicializa o sistema RAG"""
        self.cache = {}  # Cache simples mas eficaz
        logger.info("SimpleRAG inicializado")
    
    def get_relevant_context(self, user_message: str, risk_level: str = 'low', 
                           limit: int = 3) -> Optional[str]:
        """
        Busca contexto relevante de conversas bem-sucedidas
        
        Args:
            user_message: Mensagem do usu√°rio
            risk_level: N√≠vel de risco (low, moderate, high, critical)
            limit: N√∫mero m√°ximo de exemplos a retornar
            
        Returns:
            String com contexto relevante ou None se n√£o encontrar
        """
        try:
            # Verificar cache primeiro
            cache_key = f"{hash(user_message)}_{risk_level}_{limit}"
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            # Extrair palavras-chave da mensagem
            keywords = self._extract_keywords(user_message)
            
            # Buscar conversas similares
            similar_conversations = self._find_similar_conversations(keywords, risk_level, limit * 2)
            
            # Ranquear por relev√¢ncia
            ranked_conversations = self._rank_conversations(similar_conversations, user_message)
            
            # Construir contexto a partir das melhores
            context = self._build_context(ranked_conversations[:limit])
            
            # Cachear resultado
            self.cache[cache_key] = context
            
            return context
            
        except Exception as e:
            logger.error(f"Erro no RAG: {e}")
            return None
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extrai palavras-chave relevantes para sa√∫de mental"""
        mental_health_keywords = [
            'ansiedade', 'depress√£o', 'tristeza', 'medo', 'preocupa√ß√£o',
            'estresse', 'p√¢nico', 'ang√∫stia', 'solid√£o', 'vazio', 'raiva',
            'trabalho', 'fam√≠lia', 'relacionamento', 'escola', 'faculdade',
            'dinheiro', 'sa√∫de', 'futuro', 'passado', 'culpa', 'frustra√ß√£o'
        ]
        
        text_lower = text.lower()
        found_keywords = []
        
        # Buscar palavras-chave espec√≠ficas
        for keyword in mental_health_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        # Adicionar palavras importantes do texto (> 4 caracteres)
        words = [w for w in text_lower.split() if len(w) > 4 and w.isalpha()]
        found_keywords.extend(words[:5])
        
        return list(set(found_keywords))[:10]  # Remover duplicatas e limitar
    
    def _find_similar_conversations(self, keywords: List[str], risk_level: str, 
                                  limit: int = 10) -> List[Dict]:
        """Busca conversas similares no banco de dados"""
        try:
            from app import db
            
            if keywords:
                keyword_pattern = '|'.join([k for k in keywords if len(k) > 3])
            else:
                keyword_pattern = ''
            
            # Query otimizada para buscar conversas bem-sucedidas
            query = text("""
                SELECT DISTINCT
                    cm_user.content as user_message,
                    cm_ai.content as ai_response,
                    cs.user_rating,
                    cs.initial_risk_level,
                    cm_ai.created_at
                FROM chat_sessions cs
                JOIN chat_messages cm_user ON cs.id = cm_user.session_id 
                    AND cm_user.message_type = 'USER'
                JOIN chat_messages cm_ai ON cs.id = cm_ai.session_id 
                    AND cm_ai.message_type = 'AI'
                    AND cm_ai.id > cm_user.id
                WHERE 
                    cs.user_rating >= 4  -- Apenas conversas bem avaliadas
                    AND (cs.initial_risk_level = :risk_level OR :risk_level = 'any')
                    AND (:keyword_pattern = '' OR cm_user.content ~* :keyword_pattern)
                    AND cm_user.created_at >= NOW() - INTERVAL '6 months'
                    AND LENGTH(cm_user.content) > 10
                    AND LENGTH(cm_ai.content) > 20
                ORDER BY cs.user_rating DESC, cm_ai.created_at DESC
                LIMIT :limit
            """)
            
            result = db.session.execute(query, {
                'risk_level': risk_level,
                'keyword_pattern': keyword_pattern,
                'limit': limit
            })
            
            return [dict(row._mapping) for row in result]
            
        except Exception as e:
            logger.error(f"Erro na busca de conversas: {e}")
            return []
    
    def _rank_conversations(self, conversations: List[Dict], user_message: str) -> List[Dict]:
        """Ranqueia conversas por relev√¢ncia"""
        if not conversations:
            return []
        
        user_words = set(user_message.lower().split())
        
        for conv in conversations:
            try:
                conv_words = set(conv['user_message'].lower().split())
                common_words = len(user_words.intersection(conv_words))
                word_similarity = common_words / max(len(user_words), 1)
                rating_score = (conv.get('user_rating', 3)) / 5.0
                
                # Score final ponderado
                conv['relevance_score'] = (word_similarity * 0.6) + (rating_score * 0.4)
                
            except Exception:
                conv['relevance_score'] = 0
        
        conversations.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        return conversations
    
    def _build_context(self, conversations: List[Dict]) -> Optional[str]:
        """Constr√≥i contexto formatado a partir das melhores conversas"""
        if not conversations:
            return None
        
        context_parts = ["EXEMPLOS DE RESPOSTAS EFICAZES:\n"]
        
        for i, conv in enumerate(conversations, 1):
            user_msg = conv['user_message'][:120]
            ai_response = conv['ai_response'][:200]
            rating = conv.get('user_rating', 0)
            
            context_parts.append(
                f"Exemplo {i} (‚≠ê{rating}/5):\n"
                f"Situa√ß√£o: {user_msg}{'...' if len(conv['user_message']) > 120 else ''}\n"
                f"Resposta eficaz: {ai_response}{'...' if len(conv['ai_response']) > 200 else ''}\n"
            )
        
        context_parts.append("\nüí° Use como inspira√ß√£o, mas personalize para o contexto atual.")
        return "\n".join(context_parts)


class AIService:
    """
    Servi√ßo Completo de IA para Suporte Emocional
    
    Funcionalidades principais:
    - An√°lise de sentimentos com OpenAI
    - Avalia√ß√£o de n√≠veis de risco integrada
    - Gera√ß√£o de respostas emp√°ticas contextualizadas  
    - Sistema RAG para melhor qualidade
    - Sistema de prompts modular e manuten√≠vel
    - Fallback para m√∫ltiplos modelos
    - Cache e otimiza√ß√µes de performance
    
    Vers√£o: 2.1 - Modular com Prompt Manager Separado
    """
    
    def __init__(self, app=None):
        """
        Inicializa o servi√ßo de IA com todas as funcionalidades
        
        Args:
            app: Inst√¢ncia da aplica√ß√£o Flask (opcional)
        """
        self.app = app
        
        # === CONFIGURA√á√ÉO OPENAI (Principal) ===
        self.openai_client = None
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", 300))
        self.temperature = float(os.getenv("OPENAI_TEMPERATURE", 0.7))
        
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            self.openai_client = openai
            self.openai_client.api_key = openai_api_key
            logger.info("OpenAI configurado com sucesso")
        else:
            logger.warning("OpenAI API key n√£o encontrada")
        
        # === CONFIGURA√á√ÉO GEMINI (Fallback) ===
        self.gemini_client = None
        self.gemini_model = os.getenv("GEMINI_MODEL", "gemini-pro")
        
        if GEMINI_AVAILABLE:
            gemini_api_key = os.getenv("GEMINI_API_KEY")
            if gemini_api_key:
                try:
                    genai.configure(api_key=gemini_api_key)
                    self.gemini_client = genai
                    logger.info("Gemini configurado como fallback")
                except Exception as e:
                    logger.error(f"Erro ao configurar Gemini: {e}")
        

        
        # === SISTEMA RAG INTEGRADO ===
        self.rag = SimpleRAG()  # RAG b√°sico para compatibilidade
        self.rag_enabled = True
        self.advanced_rag = advanced_rag_service  # RAG avan√ßado
        
        # === SISTEMA DE PROMPTS INTEGRADO ===
        self.prompt_manager = AIPromptManager()
        self.advanced_prompt_engineer = advanced_prompt_engineer  # Prompt engineering avan√ßado
        
        # === SISTEMA DE FINE-TUNING ===
        self.finetuning_preparator = finetuning_preparator
        
        # === SISTEMA DE LOGGING DE TREINAMENTO ===
        self.training_logger = training_usage_logger
        self.log_training_usage = True
        
        # === CACHE E OTIMIZA√á√ïES ===
        self.response_cache = {}
        self.cache_max_size = 100
        
        logger.info("AIService v2.0 inicializado com sucesso")
    
    def analyze_sentiment(self, text: str) -> Dict:
        """
        Analisa o sentimento do texto usando OpenAI com fallback inteligente
        
        Args:
            text: Texto para an√°lise
            
        Returns:
            Dict com score, confidence, emotion e intensity
        """
        if not self.openai_client:
            return self._basic_sentiment_analysis(text)
        
        try:
            # Obter prompt do sistema de prompts
            system_content = self.prompt_manager.get_sentiment_prompt('openai')
            
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": f"Analise: {text}"}
                ],
                max_tokens=150,
                temperature=0.3
            )
            
            content = response.choices[0].message.content.strip()
            result = json.loads(content)
            
            # Validar resultado
            if all(key in result for key in ['score', 'confidence', 'emotion', 'intensity']):
                return result
            else:
                return self._basic_sentiment_analysis(text)
                
        except Exception as e:
            logger.error(f"Erro na an√°lise OpenAI: {e}")
            return self._basic_sentiment_analysis(text)
    
    def _basic_sentiment_analysis(self, text: str) -> Dict:
        """An√°lise b√°sica de sentimento como fallback"""
        text_lower = text.lower()
        
        positive_words = ['bem', 'bom', 'feliz', 'alegre', '√≥timo', 'amor', 'paz']
        negative_words = ['mal', 'ruim', 'triste', 'deprimido', '√≥dio', 'raiva', 'medo']
        critical_words = ['morrer', 'matar', 'suic√≠dio', 'acabar', 'desistir']
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        critical_count = sum(1 for word in critical_words if word in text_lower)
        
        total_words = max(len(text_lower.split()), 1)
        score = (positive_count - negative_count - critical_count * 2) / total_words
        score = max(-1, min(1, score))
        
        if critical_count > 0:
            emotion, intensity = 'desesperado', 'high'
        elif score < -0.3:
            emotion = 'triste'
            intensity = 'high' if score < -0.6 else 'moderate'
        elif score > 0.3:
            emotion, intensity = 'feliz', 'moderate'
        else:
            emotion, intensity = 'neutro', 'low'
        
        return {
            'score': score,
            'confidence': 0.6,
            'emotion': emotion,
            'intensity': intensity
        }
    
    def assess_risk_level(self, text: str, sentiment_analysis: Optional[Dict] = None) -> str:
        """
        Avalia n√≠vel de risco usando RiskAnalyzer integrado
        
        Args:
            text: Texto para an√°lise
            sentiment_analysis: An√°lise de sentimento pr√©via (opcional)
            
        Returns:
            N√≠vel de risco: 'low', 'moderate', 'high', 'critical'
        """
        try:
            from app.services.risk_analyzer import RiskAnalyzer
            risk_analyzer = RiskAnalyzer()
            analysis = risk_analyzer.analyze_message(text)
            return analysis.get('risk_level', 'low')
        except ImportError:
            logger.warning("RiskAnalyzer n√£o dispon√≠vel, usando an√°lise b√°sica")
            return self._basic_risk_assessment(text, sentiment_analysis)
    
    def _basic_risk_assessment(self, text: str, sentiment_analysis: Optional[Dict] = None) -> str:
        """Avalia√ß√£o b√°sica de risco como fallback"""
        text_lower = text.lower()
        
        critical_keywords = [
            'me matar', 'suic√≠dio', 'quero morrer', 'acabar com tudo',
            'n√£o quero viver', 'melhor morto'
        ]
        
        high_keywords = [
            'n√£o aguento mais', 'desesperado', 'sem esperan√ßa', 'vazio total',
            'depress√£o severa', 'ansiedade extrema'
        ]
        
        moderate_keywords = [
            'triste', 'ansioso', 'preocupado', 'dif√≠cil', 'problema', 'ajuda'
        ]
        
        # Verificar palavras cr√≠ticas
        for keyword in critical_keywords:
            if keyword in text_lower:
                return 'critical'
        
        # Usar an√°lise de sentimento se dispon√≠vel
        if sentiment_analysis:
            score = sentiment_analysis.get('score', 0)
            emotion = sentiment_analysis.get('emotion', '')
            
            if emotion == 'desesperado' or score < -0.8:
                return 'critical'
            elif score < -0.5:
                return 'high'
        
        # Verificar outras palavras-chave
        for keyword in high_keywords:
            if keyword in text_lower:
                return 'high'
        
        for keyword in moderate_keywords:
            if keyword in text_lower:
                return 'moderate'
        
        return 'low'
        
        # Verificar palavras cr√≠ticas
        for keyword in critical_keywords:
            if keyword in text_lower:
                return 'critical'
        
        # Usar an√°lise de sentimento se dispon√≠vel
        if sentiment_analysis:
            score = sentiment_analysis.get('score', 0)
            emotion = sentiment_analysis.get('emotion', '').lower()
            intensity = sentiment_analysis.get('intensity', '').lower()
            
            if emotion == 'desesperado' and intensity == 'high':
                return 'critical'
            elif emotion in ['triste', 'vazio'] and intensity == 'high' and score < -0.7:
                return 'high'
        
        # Verificar outras palavras-chave
        for keyword in high_keywords:
            if keyword in text_lower:
                return 'high'
        
        for keyword in moderate_keywords:
            if keyword in text_lower:
                return 'moderate'
        
        return 'low'
    
    def analyze_with_risk_assessment(self, text: str) -> Dict:
        """
        An√°lise completa: sentimento + risco em uma chamada otimizada
        
        Args:
            text: Texto para an√°lise completa
            
        Returns:
            Dict com an√°lise completa
        """
        try:
            # Verificar cache primeiro
            cache_key = f"analysis_{hash(text)}"
            if cache_key in self.response_cache:
                return self.response_cache[cache_key]
            
            # An√°lise de sentimento
            sentiment_result = self.analyze_sentiment(text)
            
            # Avalia√ß√£o de risco
            risk_level = self.assess_risk_level(text, sentiment_result)
            
            # Combinar resultados
            sentiment_result.update({
                'risk_level': risk_level,
                'requires_attention': risk_level in ['high', 'critical'],
                'timestamp': datetime.now(UTC).isoformat()
            })
            
            # Cachear resultado
            self._cache_result(cache_key, sentiment_result)
            
            return sentiment_result
            
        except Exception as e:
            logger.error(f"Erro na an√°lise completa: {e}")
            return {
                'score': 0,
                'confidence': 0.1,
                'emotion': 'neutro',
                'intensity': 'low',
                'risk_level': self.assess_risk_level(text),
                'requires_attention': False,
                'error': str(e),
                'timestamp': datetime.now(UTC).isoformat()
            }
    
    def analyze_diary_entry(self, entry_content: str) -> Optional[Dict]:
        """
        Analisa uma entrada do di√°rio para extrair insights emocionais
        
        Args:
            entry_content: Conte√∫do da entrada do di√°rio
            
        Returns:
            Dict com an√°lise da entrada ou None em caso de erro
        """
        try:
            if not entry_content or not entry_content.strip():
                return None
            
            # An√°lise de sentimento b√°sica
            sentiment_result = self.analyze_sentiment(entry_content)
            
            # Detectar emo√ß√µes espec√≠ficas usando palavras-chave
            emotions = self._detect_emotions_in_text(entry_content)
            
            # Identificar indicadores de risco
            risk_indicators = self._identify_risk_indicators(entry_content)
            
            # Extrair temas principais
            themes = self._extract_themes(entry_content)
            
            # Compilar an√°lise completa
            analysis = {
                'sentiment_score': sentiment_result.get('score', 0),
                'primary_emotion': sentiment_result.get('emotion', 'neutro'),
                'emotional_intensity': sentiment_result.get('intensity', 'low'),
                'detected_emotions': emotions,
                'risk_indicators': risk_indicators,
                'main_themes': themes,
                'analysis_timestamp': datetime.now(UTC).isoformat(),
                'word_count': len(entry_content.split()),
                'requires_attention': len(risk_indicators) > 0 or sentiment_result.get('score', 0) < -0.6
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Erro na an√°lise do di√°rio: {e}")
            return None
    
    def _detect_emotions_in_text(self, text: str) -> List[str]:
        """Detecta emo√ß√µes espec√≠ficas no texto usando palavras-chave"""
        emotion_keywords = {
            'alegria': ['feliz', 'alegre', 'contente', 'animado', 'euf√≥rico', 'radiante', 'otimista'],
            'tristeza': ['triste', 'melanc√≥lico', 'deprimido', 'abatido', 'desanimado', 'lamentando'],
            'ansiedade': ['ansioso', 'nervoso', 'preocupado', 'inquieto', 'apreensivo', 'tenso'],
            'raiva': ['raiva', 'irritado', 'furioso', 'bravo', 'indignado', 'revoltado'],
            'medo': ['medo', 'amedrontado', 'assustado', 'aterrorizado', 'receoso'],
            'gratid√£o': ['grato', 'agradecido', 'reconhecido', 'aben√ßoado'],
            'amor': ['amor', 'carinho', 'afeto', 'paix√£o', 'adora√ß√£o'],
            'esperan√ßa': ['esperan√ßa', 'esperan√ßoso', 'confiante', 'positivo'],
            'solid√£o': ['sozinho', 'isolado', 'abandonado', 'solit√°rio'],
            'culpa': ['culpa', 'culpado', 'arrependido', 'remorso']
        }
        
        text_lower = text.lower()
        detected_emotions = []
        
        for emotion, keywords in emotion_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                detected_emotions.append(emotion)
        
        return detected_emotions[:3]  # M√°ximo 3 emo√ß√µes principais
    
    def _identify_risk_indicators(self, text: str) -> List[str]:
        """Identifica indicadores de risco no texto"""
        risk_keywords = {
            'suicidal_ideation': ['suic√≠dio', 'acabar com tudo', 'n√£o aguento mais', 'quero morrer'],
            'self_harm': ['me machucar', 'me cortar', 'autoles√£o', 'me ferir'],
            'substance_abuse': ['bebendo muito', 'usando drogas', 'viciado', 'dependente'],
            'eating_disorder': ['n√£o como', 'vomito', 'bulimia', 'anorexia'],
            'severe_depression': ['n√£o vejo sentido', 'vida sem prop√≥sito', 'desesperan√ßa total'],
            'panic_attacks': ['p√¢nico', 'cora√ß√£o acelerado', 'n√£o consigo respirar', 'ataque'],
            'social_isolation': ['n√£o saio', 'evito pessoas', 'me isolo', 'ningu√©m me entende']
        }
        
        text_lower = text.lower()
        risk_indicators = []
        
        for risk_type, keywords in risk_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                risk_indicators.append(risk_type)
        
        return risk_indicators
    
    def _extract_themes(self, text: str) -> List[str]:
        """Extrai temas principais do texto"""
        theme_keywords = {
            'trabalho': ['trabalho', 'emprego', 'chefe', 'colega', 'carreira', 'profissional'],
            'fam√≠lia': ['fam√≠lia', 'pai', 'm√£e', 'irm√£o', 'filho', 'parente'],
            'relacionamento': ['namorado', 'namorada', 'marido', 'esposa', 'relacionamento', 'amor'],
            'estudos': ['escola', 'universidade', 'faculdade', 'estudo', 'prova', 'professor'],
            'sa√∫de': ['sa√∫de', 'doen√ßa', 'm√©dico', 'hospital', 'tratamento', 'dor'],
            'finan√ßas': ['dinheiro', 'financeiro', 'conta', 'pagamento', 'd√≠vida', 'sal√°rio'],
            'amizade': ['amigo', 'amiga', 'amizade', 'colega', 'turma'],
            'futuro': ['futuro', 'planos', 'objetivos', 'sonhos', 'metas', 'amanh√£'],
            'autoestima': ['autoestima', 'confian√ßa', 'inseguro', 'valor pr√≥prio', 'autoimagem']
        }
        
        text_lower = text.lower()
        themes = []
        
        for theme, keywords in theme_keywords.items():
            keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
            if keyword_count >= 2:  # Pelo menos 2 palavras-chave do tema
                themes.append(theme)
            elif keyword_count == 1 and len([k for k in keywords if k in text_lower and len(k) > 6]) > 0:
                # Uma palavra-chave espec√≠fica/longa
                themes.append(theme)
        
        return themes[:5]  # M√°ximo 5 temas
    
    def generate_response(self, user_message: str, risk_level: str = 'low', 
                         user_context: Optional[Dict] = None, 
                         conversation_history: Optional[List] = None, 
                         fallback: bool = True) -> Dict:
        """
        Gera resposta emp√°tica usando LLMs com RAG avan√ßado e Prompt Engineering
        
        Args:
            user_message: Mensagem do usu√°rio
            risk_level: N√≠vel de risco detectado
            user_context: Contexto do usu√°rio (nome, etc.)
            conversation_history: Hist√≥rico da conversa
            fallback: Se deve usar fallback em caso de erro
            
        Returns:
            Dict com resposta gerada e metadados
        """
        errors = []
        
        try:
            print(f"AI_RESPONSE_START: Processando mensagem de risco {risk_level}")
            
            # 1. Buscar contexto avan√ßado usando RAG
            rag_context = None
            if self.rag_enabled:
                try:
                    rag_result = self.advanced_rag.get_enhanced_context(
                        user_message, 
                        risk_level, 
                        context_type='all', 
                        limit=3
                    )
                    rag_context = rag_result.get('context_prompt', '')
                    print(f"RAG_CONTEXT: {len(rag_result.get('training_data', []))} dados + {len(rag_result.get('conversation_examples', []))} conversas")
                except Exception as e:
                    logger.warning(f"Erro no RAG avan√ßado: {e}")
            
            # 2. Preparar contexto para prompt engineering
            prompt_context = PromptContext(
                user_message=user_message,
                risk_level=RiskLevel(risk_level),
                user_name=user_context.get('name') if user_context else None,
                session_history=conversation_history,
                training_context=rag_context,
                conversation_examples=rag_result.get('conversation_examples', []) if rag_context else None
            )
            
            # 3. Tentar OpenAI com prompt engineering avan√ßado
            if self.openai_client:
                try:
                    return self._generate_response_openai_advanced(prompt_context)
                except Exception as e:
                    errors.append(f"OpenAI avan√ßado: {str(e)}")
                    logger.warning(f"Falha no OpenAI avan√ßado: {e}")
            
            # 4. Tentar Gemini com prompt engineering avan√ßado (fallback)
            if self.gemini_client and fallback:
                try:
                    return self._generate_response_gemini_advanced(prompt_context)
                except Exception as e:
                    errors.append(f"Gemini avan√ßado: {str(e)}")
                    logger.warning(f"Falha no Gemini avan√ßado: {e}")
            
            # 5. Fallback para sistema antigo
            print("FALLBACK_TO_OLD: Usando sistema de resposta legado")
            return self._generate_response_legacy(user_message, risk_level, user_context, conversation_history, errors)
            
        except Exception as e:
            logger.error(f"Erro cr√≠tico na gera√ß√£o de resposta: {e}")
            return self._generate_response_fallback(user_message, risk_level, user_context, errors + [str(e)])
    
    def _generate_response_openai_advanced(self, context: PromptContext) -> Dict:
        """Gera resposta usando OpenAI com sistema avan√ßado"""
        
        # Construir prompt usando prompt engineering avan√ßado
        prompt_data = self.advanced_prompt_engineer.build_contextual_prompt(
            context, provider='openai'
        )
        
        print(f"OPENAI_ADVANCED: Prompt com {len(prompt_data['messages'])} mensagens")
        
        # Chamada para OpenAI
        response = self.openai_client.chat.completions.create(
            model=self.openai_model,
            messages=prompt_data['messages'],
            max_tokens=prompt_data['max_tokens'],
            temperature=prompt_data['temperature'],
            presence_penalty=prompt_data.get('presence_penalty', 0),
            frequency_penalty=prompt_data.get('frequency_penalty', 0)
        )
        
        ai_response = response.choices[0].message.content.strip()
        
        # Log do uso de dados de treinamento
        training_usage = None
        if self.log_training_usage:
            try:
                training_usage = log_ai_response_with_training_check(
                    context.user_message, ai_response, context.risk_level.value
                )
                print(f"TRAINING_USAGE: {training_usage.get('used_training_data', False)}")
            except Exception as e:
                logger.warning(f"Erro ao logar uso de treinamento: {e}")
        
        result = {
            'message': ai_response,
            'risk_level': context.risk_level.value,
            'confidence': 0.95,  # Alta confian√ßa com sistema avan√ßado
            'source': 'openai_advanced',
            'model': self.openai_model,
            'rag_used': bool(context.training_context),
            'prompt_engineering': 'advanced',
            'timestamp': datetime.now(UTC).isoformat(),
            'tokens_used': response.usage.total_tokens if hasattr(response, 'usage') else None
        }
        
        # Adicionar informa√ß√µes de treinamento se dispon√≠vel
        if training_usage:
            result['training_usage'] = training_usage
        
        # Adicionar contexto RAG se usado
        if context.training_context:
            result['rag_context_length'] = len(context.training_context)
        
        print(f"OPENAI_SUCCESS: Resposta gerada com {len(ai_response)} caracteres")
        return result
    
    def _generate_response_gemini_advanced(self, context: PromptContext) -> Dict:
        """Gera resposta usando Gemini com sistema avan√ßado"""
        
        # Construir prompt usando prompt engineering avan√ßado
        prompt_data = self.advanced_prompt_engineer.build_contextual_prompt(
            context, provider='gemini'
        )
        
        print(f"GEMINI_ADVANCED: Prompt com {len(prompt_data['prompt'])} caracteres")
        
        model = self.gemini_client.GenerativeModel(self.gemini_model)
        response = model.generate_content(prompt_data['prompt'])
        
        ai_response = response.text.strip()
        
        # Log do uso de dados de treinamento
        training_usage = None
        if self.log_training_usage:
            try:
                training_usage = log_ai_response_with_training_check(
                    context.user_message, ai_response, context.risk_level.value
                )
                print(f"TRAINING_USAGE: {training_usage.get('used_training_data', False)}")
            except Exception as e:
                logger.warning(f"Erro ao logar uso de treinamento: {e}")
        
        result = {
            'message': ai_response,
            'risk_level': context.risk_level.value,
            'confidence': 0.90,  # Boa confian√ßa com sistema avan√ßado
            'source': 'gemini_advanced',
            'rag_used': bool(context.training_context),
            'prompt_engineering': 'advanced',
            'timestamp': datetime.now(UTC).isoformat()
        }
        
        # Adicionar informa√ß√µes de treinamento se dispon√≠vel
        if training_usage:
            result['training_usage'] = training_usage
        
        # Adicionar contexto RAG se usado
        if context.training_context:
            result['rag_context_length'] = len(context.training_context)
        
        print(f"GEMINI_SUCCESS: Resposta gerada com {len(ai_response)} caracteres")
        return result
    
    def _generate_response_legacy(self, user_message: str, risk_level: str, 
                                 user_context: Optional[Dict], 
                                 conversation_history: Optional[List],
                                 errors: List[str]) -> Dict:
        """M√©todo legado para compatibilidade"""
        
        print("LEGACY_RESPONSE: Usando sistema antigo")
        
        # Buscar contexto RAG b√°sico se habilitado
        rag_context = None
        if self.rag_enabled:
            try:
                rag_context = self.rag.get_relevant_context(user_message, risk_level)
            except Exception as e:
                logger.warning(f"Erro no RAG b√°sico: {e}")
        
        # Usar sistema antigo com RAG b√°sico
        is_first_message = not conversation_history or len(conversation_history) == 0
        
        # Construir prompt usando o sistema de prompts antigo
        prompt_data = self.prompt_manager.build_conversation_prompt(
            user_message=user_message,
            risk_level=risk_level,
            provider='openai',
            user_context=user_context,
            conversation_history=conversation_history,
            rag_context=rag_context,
            is_first_message=is_first_message
        )
        
        # Tentar OpenAI
        if self.openai_client:
            try:
                response = self.openai_client.chat.completions.create(
                    model=self.openai_model,
                    messages=prompt_data['messages'],
                    max_tokens=prompt_data['max_tokens'],
                    temperature=prompt_data['temperature']
                )
                
                ai_response = response.choices[0].message.content.strip()
                
                return {
                    'message': ai_response,
                    'risk_level': risk_level,
                    'confidence': 0.85,
                    'source': 'openai_legacy',
                    'model': self.openai_model,
                    'rag_used': bool(rag_context),
                    'prompt_engineering': 'legacy',
                    'timestamp': datetime.now(UTC).isoformat(),
                    'fallback_errors': errors
                }
                
            except Exception as e:
                errors.append(f"OpenAI legacy: {str(e)}")
        
        # Fallback final
        return self._generate_response_fallback(user_message, risk_level, user_context, errors)
    
    def _generate_response_openai(self, user_message: str, risk_level: str, 
                                 user_context: Optional[Dict], 
                                 conversation_history: Optional[List],
                                 rag_context: Optional[str]) -> Dict:
        """Gera resposta usando OpenAI com contexto RAG"""
        
        # Verificar se √© primeira mensagem
        is_first_message = not conversation_history or len(conversation_history) == 0
        
        # Construir prompt usando o sistema de prompts
        prompt_data = self.prompt_manager.build_conversation_prompt(
            user_message=user_message,
            risk_level=risk_level,
            provider='openai',
            user_context=user_context,
            conversation_history=conversation_history,
            rag_context=rag_context,
            is_first_message=is_first_message
        )
        
        # Chamada para OpenAI
        response = self.openai_client.chat.completions.create(
            model=self.openai_model,
            messages=prompt_data['messages'],
            max_tokens=prompt_data['max_tokens'],
            temperature=prompt_data['temperature']
        )
        
        ai_response = response.choices[0].message.content.strip()
        
        # Log do uso de dados de treinamento
        training_usage = None
        if self.log_training_usage:
            try:
                training_usage = log_ai_response_with_training_check(
                    user_message, ai_response, risk_level
                )
            except Exception as e:
                logger.warning(f"Erro ao logar uso de treinamento: {e}")
        
        result = {
            'message': ai_response,
            'risk_level': risk_level,
            'confidence': 0.9,
            'source': 'openai',
            'model': self.openai_model,
            'rag_used': bool(rag_context),
            'timestamp': datetime.now(UTC).isoformat()
        }
        
        # Adicionar informa√ß√µes de treinamento se dispon√≠vel
        if training_usage:
            result['training_usage'] = training_usage
        
        return result
    
    def _generate_response_gemini(self, user_message: str, risk_level: str, 
                                 user_context: Optional[Dict],
                                 rag_context: Optional[str]) -> Dict:
        """Gera resposta usando Gemini (fallback)"""
        
        # Construir prompt usando o sistema de prompts
        prompt_data = self.prompt_manager.build_conversation_prompt(
            user_message=user_message,
            risk_level=risk_level,
            provider='gemini',
            user_context=user_context,
            rag_context=rag_context,
            is_first_message=True  # Gemini n√£o mant√©m estado
        )
        
        model = self.gemini_client.GenerativeModel(self.gemini_model)
        response = model.generate_content(prompt_data['prompt'])
        
        ai_response = response.text.strip()
        
        # Log do uso de dados de treinamento
        training_usage = None
        if self.log_training_usage:
            try:
                training_usage = log_ai_response_with_training_check(
                    user_message, ai_response, risk_level
                )
            except Exception as e:
                logger.warning(f"Erro ao logar uso de treinamento: {e}")
        
        result = {
            'message': ai_response,
            'risk_level': risk_level,
            'confidence': 0.85,
            'source': 'gemini',
            'rag_used': bool(rag_context),
            'timestamp': datetime.now(UTC).isoformat()
        }
        
        # Adicionar informa√ß√µes de treinamento se dispon√≠vel
        if training_usage:
            result['training_usage'] = training_usage
        
        return result
    
    def _generate_response_fallback(self, user_message: str, risk_level: str, 
                                   user_context: Optional[Dict], 
                                   errors: Optional[List] = None) -> Dict:
        """Gera resposta est√°tica como fallback final"""
        
        # Obter respostas do sistema de prompts
        responses = self.prompt_manager.get_fallback_responses(risk_level, user_context)
        message = random.choice(responses)
        
        return {
            'message': message,
            'risk_level': risk_level,
            'confidence': 0.3,
            'source': 'fallback',
            'errors': errors or [],
            'timestamp': datetime.now(UTC).isoformat()
        }
    
    def _cache_result(self, key: str, result: Dict) -> None:
        """Gerencia cache com limpeza autom√°tica"""
        try:
            if len(self.response_cache) >= self.cache_max_size:
                # Remove primeiros 20% dos itens
                items_to_remove = list(self.response_cache.keys())[:self.cache_max_size // 5]
                for item_key in items_to_remove:
                    del self.response_cache[item_key]
            
            self.response_cache[key] = result
        except Exception as e:
            logger.warning(f"Erro ao cachear: {e}")
    
    def get_model_info(self) -> List[Dict]:
        """Retorna informa√ß√µes sobre modelos configurados"""
        return [
            {
                'provider': 'OpenAI',
                'model': self.openai_model,
                'status': 'active' if self.openai_client else 'inactive',
                'type': 'cloud',
                'primary': True
            },
            {
                'provider': 'Gemini',
                'model': self.gemini_model,
                'status': 'active' if self.gemini_client else 'inactive',
                'type': 'cloud',
                'primary': False
            },
            {
                'provider': 'RAG',
                'model': 'SimpleRAG',
                'status': 'active' if self.rag_enabled else 'inactive',
                'type': 'enhancement',
                'primary': False
            },
            {
                'provider': 'PromptManager',
                'model': 'AIPromptManager',
                'status': 'active',
                'type': 'prompt_system',
                'primary': False
            },
            {
                'provider': 'Fallback',
                'model': 'static_responses',
                'status': 'always',
                'type': 'static',
                'primary': False
            }
        ]
    
    def get_service_statistics(self) -> Dict:
        """Retorna estat√≠sticas do servi√ßo"""
        try:
            stats = {
                'service_version': '3.0',
                'architecture': 'advanced_integrated_system',
                'models_active': len([m for m in self.get_model_info() if m['status'] == 'active']),
                'cache_size': len(self.response_cache),
                'rag_enabled': self.rag_enabled,
                'prompt_manager_active': bool(self.prompt_manager),
                'training_logging_enabled': self.log_training_usage,
                'advanced_systems': {
                    'advanced_rag': bool(self.advanced_rag),
                    'advanced_prompt_engineer': bool(self.advanced_prompt_engineer),
                    'finetuning_preparator': bool(self.finetuning_preparator)
                }
            }
            
            # Estat√≠sticas do RAG avan√ßado
            if self.advanced_rag:
                try:
                    rag_stats = self.advanced_rag.get_training_data_statistics()
                    stats['advanced_rag_stats'] = rag_stats
                except Exception as e:
                    stats['advanced_rag_error'] = str(e)
            
            # Estat√≠sticas do prompt engineering
            if self.advanced_prompt_engineer:
                try:
                    prompt_stats = self.advanced_prompt_engineer.get_prompt_statistics()
                    stats['prompt_engineering_stats'] = prompt_stats
                except Exception as e:
                    stats['prompt_engineering_error'] = str(e)
            
            # Estat√≠sticas do fine-tuning
            if self.finetuning_preparator:
                try:
                    ft_recommendations = self.finetuning_preparator.get_dataset_recommendations()
                    stats['finetuning_recommendations'] = ft_recommendations
                except Exception as e:
                    stats['finetuning_error'] = str(e)
            
            # Estat√≠sticas de uso de treinamento
            if self.log_training_usage:
                try:
                    training_stats = self.training_logger.get_usage_statistics()
                    stats['training_usage_stats'] = training_stats
                except Exception as e:
                    stats['training_usage_error'] = str(e)
            
            return stats
        except Exception as e:
            return {'error': str(e)}
    
    def search_training_content(self, query: str, limit: int = 10) -> Dict:
        """
        Busca conte√∫do nos dados de treinamento
        """
        try:
            if not self.advanced_rag:
                return {'error': 'Sistema RAG avan√ßado n√£o dispon√≠vel'}
            
            results = self.advanced_rag.search_training_content(query, limit)
            
            return {
                'success': True,
                'query': query,
                'results': results,
                'total_found': len(results)
            }
            
        except Exception as e:
            logger.error(f"Erro na busca de conte√∫do: {e}")
            return {'error': str(e)}
    
    def create_finetuning_dataset(self, dataset_type: str = 'hybrid', 
                                 format_type: str = 'openai_chat',
                                 max_samples: int = 1000) -> Dict:
        """
        Cria dataset para fine-tuning
        
        Args:
            dataset_type: Tipo do dataset ('conversations', 'training_data', 'hybrid')
            format_type: Formato ('openai_chat', 'jsonl', 'csv')
            max_samples: N√∫mero m√°ximo de amostras
        """
        try:
            if not self.finetuning_preparator:
                return {'error': 'Preparador de fine-tuning n√£o dispon√≠vel'}
            
            print(f"FINETUNING_DATASET: Criando dataset {dataset_type} formato {format_type}")
            
            if dataset_type == 'conversations':
                result = self.finetuning_preparator.create_conversation_dataset(
                    format_type=format_type,
                    max_samples=max_samples
                )
            elif dataset_type == 'training_data':
                result = self.finetuning_preparator.create_training_data_dataset(
                    format_type=format_type,
                    max_samples=max_samples
                )
            elif dataset_type == 'hybrid':
                result = self.finetuning_preparator.create_hybrid_dataset(
                    total_samples=max_samples,
                    format_type=format_type
                )
            else:
                return {'error': f'Tipo de dataset n√£o suportado: {dataset_type}'}
            
            return result
            
        except Exception as e:
            logger.error(f"Erro na cria√ß√£o do dataset: {e}")
            return {'error': str(e)}
    
    def save_finetuning_dataset(self, dataset_result: Dict, file_path: str = None) -> Dict:
        """
        Salva dataset de fine-tuning em arquivo
        """
        try:
            if not self.finetuning_preparator:
                return {'error': 'Preparador de fine-tuning n√£o dispon√≠vel'}
            
            return self.finetuning_preparator.save_dataset_to_file(dataset_result, file_path)
            
        except Exception as e:
            logger.error(f"Erro ao salvar dataset: {e}")
            return {'error': str(e)}
    
    def get_advanced_context(self, user_message: str, risk_level: str = 'low',
                           context_type: str = 'all') -> Dict:
        """
        Obt√©m contexto avan√ßado usando RAG
        """
        try:
            if not self.advanced_rag:
                return {'error': 'Sistema RAG avan√ßado n√£o dispon√≠vel'}
            
            return self.advanced_rag.get_enhanced_context(
                user_message, risk_level, context_type
            )
            
        except Exception as e:
            logger.error(f"Erro no contexto avan√ßado: {e}")
            return {'error': str(e)}
    
    def analyze_prompt_context(self, user_message: str, risk_level: str = 'low',
                             user_context: Optional[Dict] = None) -> Dict:
        """
        Analisa contexto para prompt engineering
        """
        try:
            if not self.advanced_prompt_engineer:
                return {'error': 'Sistema de prompt engineering n√£o dispon√≠vel'}
            
            from .advanced_prompt_engineer import PromptContext, RiskLevel
            
            # Criar contexto
            prompt_context = PromptContext(
                user_message=user_message,
                risk_level=RiskLevel(risk_level),
                user_name=user_context.get('name') if user_context else None
            )
            
            # Validar contexto
            is_valid, errors = self.advanced_prompt_engineer.validate_prompt_context(prompt_context)
            
            # Construir prompt de exemplo
            prompt_data = self.advanced_prompt_engineer.build_contextual_prompt(
                prompt_context, provider='openai'
            )
            
            return {
                'success': True,
                'context_valid': is_valid,
                'validation_errors': errors,
                'prompt_preview': {
                    'system_message': prompt_data['messages'][0]['content'][:500] + '...',
                    'user_message': prompt_data['messages'][-1]['content'],
                    'total_messages': len(prompt_data['messages']),
                    'max_tokens': prompt_data['max_tokens'],
                    'temperature': prompt_data['temperature']
                }
            }
            
        except Exception as e:
            logger.error(f"Erro na an√°lise do prompt: {e}")
            return {'error': str(e)}
    
    def get_system_capabilities(self) -> Dict:
        """
        Retorna capacidades completas do sistema
        """
        return {
            'core_features': {
                'sentiment_analysis': True,
                'risk_assessment': True,
                'response_generation': True,
                'diary_analysis': True
            },
            'advanced_features': {
                'advanced_rag': bool(self.advanced_rag),
                'prompt_engineering': bool(self.advanced_prompt_engineer),
                'finetuning_preparation': bool(self.finetuning_preparator),
                'training_usage_logging': self.log_training_usage
            },
            'supported_formats': {
                'finetuning': ['openai_chat', 'openai_completion', 'jsonl', 'csv'],
                'rag_context': ['training_data', 'conversations', 'hybrid']
            },
            'models': self.get_model_info(),
            'version': '3.0 - Integrated Advanced System'
        }
    
    def get_training_usage_report(self) -> Dict:
        """
        Retorna relat√≥rio detalhado do uso de dados de treinamento
        """
        try:
            if not self.log_training_usage:
                return {'error': 'Logging de treinamento n√£o est√° habilitado'}
            
            return self.training_logger.get_usage_statistics()
            
        except Exception as e:
            logger.error(f"Erro ao gerar relat√≥rio de treinamento: {e}")
            return {'error': str(e)}
    
    def clear_training_cache(self) -> Dict:
        """
        Limpa o cache de dados de treinamento
        """
        try:
            if not self.log_training_usage:
                return {'error': 'Logging de treinamento n√£o est√° habilitado'}
            
            self.training_logger.clear_cache()
            return {'success': True, 'message': 'Cache de treinamento limpo'}
            
        except Exception as e:
            logger.error(f"Erro ao limpar cache de treinamento: {e}")
            return {'error': str(e)}


# === FUN√á√ïES DE CONVENI√äNCIA ===

def create_ai_service(app=None) -> AIService:
    """Cria inst√¢ncia configurada do AIService"""
    return AIService(app)

# Alias para compatibilidade
EmotionalSupportAgent = AIService

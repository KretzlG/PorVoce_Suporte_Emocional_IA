# --- Regra de transi√ß√£o de risco ---
def validate_risk_transition(previous: "RiskLevel", new: "RiskLevel") -> "RiskLevel":
    """
    Garante que n√£o haja descida abrupta de 'critical' para 'low'.
    Se anterior era 'critical', s√≥ permite 'critical' ou 'high'.
    Caso contr√°rio, retorna o anterior.
    """
    if previous == RiskLevel.CRITICAL and new in [RiskLevel.LOW, RiskLevel.MODERATE]:
        # Mant√©m 'critical' ou permite apenas 'high'
        return RiskLevel.HIGH if new == RiskLevel.HIGH else RiskLevel.CRITICAL
    return new

# Sistema de Prompts para IA de Suporte Emocional
# Centraliza templates, instru√ß√µes e configura√ß√µes de tom
# Refatorado para facilitar manuten√ß√£o e extens√£o

import logging
from typing import Dict, Optional, List, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class PromptType(Enum):
    EMPATHETIC_RESPONSE = "empathetic_response"
    CRISIS_INTERVENTION = "crisis_intervention"
    COGNITIVE_BEHAVIORAL = "cognitive_behavioral"
    MINDFULNESS_BASED = "mindfulness_based"
    SOLUTION_FOCUSED = "solution_focused"



class RiskLevel(Enum):
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"
    CRITICAL = "critical"

# --- Regra de transi√ß√£o de risco ---
def validate_risk_transition(previous: RiskLevel, new: RiskLevel) -> RiskLevel:
    """
    Garante que n√£o haja descida abrupta de 'critical' para 'low'.
    Se anterior era 'critical', s√≥ permite 'critical' ou 'high'.
    Caso contr√°rio, retorna o anterior.
    """
    if previous == RiskLevel.CRITICAL and new in [RiskLevel.LOW, RiskLevel.MODERATE]:
        # Mant√©m 'critical' ou permite apenas 'high'
        return RiskLevel.HIGH if new == RiskLevel.HIGH else RiskLevel.CRITICAL
    return new


@dataclass
class PromptContext:
    user_message: str
    risk_level: RiskLevel
    user_name: Optional[str] = None
    session_history: Optional[List[Dict]] = None
    training_context: Optional[str] = None
    conversation_examples: Optional[List[Dict]] = None
    emotional_state: Optional[str] = None
    dominant_themes: Optional[List[str]] = None
    therapeutic_approach: Optional[PromptType] = None


class AIPromptManager:
    """
    Gerenciador central dos prompts e regras de resposta da IA.
    Coment√°rios orientam manuten√ß√£o e extens√£o.
    """
    def __init__(self):
        pass
    
    # Fun√ß√£o de fallback pode ser movida para um utilit√°rio ou arquivo separado se desejar
    def get_fallback_responses(self, risk_level: str, user_context: Optional[Dict] = None) -> List[str]:
        """Retorna respostas est√°ticas por n√≠vel de risco."""
        user_name = user_context['name'].split()[0] if user_context and user_context.get('name') else ""
        name_prefix = f"{user_name}, " if user_name else ""
        fallback_responses = {
            'critical': [
                f"{name_prefix}SITUA√á√ÉO CR√çTICA DETECTADA! Nossa equipe foi acionada. üö®",
                f"{name_prefix}TRIAGEM EMERGENCIAL ATIVADA! Profissional em contato. ‚ö†Ô∏è",
                f"{name_prefix}VOC√ä N√ÉO EST√Å SOZINHO! Atendimento de crise AGORA. üÜò"
            ],
            'high': [
                f"{name_prefix}nossa triagem ir√° te atender. Voc√™ merece suporte! üí™",
                f"{name_prefix}conectando com profissionais. Voc√™ tem for√ßa! üåü",
                f"{name_prefix}protocolo de apoio intensivo ativado. Dias melhores vir√£o! ‚òÄÔ∏è"
            ],
            'moderate': [
                f"{name_prefix}suporte mais direcionado dispon√≠vel! O que pode fazer hoje? üí≠",
                f"{name_prefix}voc√™ √© forte. Vamos conectar recursos internos? üåü",
                f"{name_prefix}isso vai passar! Triagem pode organizar acompanhamento. ‚ú®"
            ],
            'low': [
                f"{name_prefix}que bom ter voc√™ aqui! Como posso apoiar? üòä",
                f"{name_prefix}oi! Como est√° se sentindo? üíö",
                f"{name_prefix}estou aqui para ouvir! O que est√° acontecendo? üó£Ô∏è"
            ]
        }
        return fallback_responses.get(risk_level, fallback_responses['low'])
    
    # M√©todos de prompt removidos. Toda l√≥gica de prompt est√° nos arquivos prompts_openai.py e prompts_gemini.py

    def get_sentiment_prompt(self, user_message: str) -> str:
        """
        Gera um prompt simples para an√°lise de sentimento da mensagem do usu√°rio.
        Pode ser customizado para diferentes modelos.
        """
        return f"Analise o sentimento da seguinte mensagem do usu√°rio e responda apenas com uma palavra (positivo, negativo ou neutro):\n{user_message}"

    def build_contextual_prompt(self, context: PromptContext, **kwargs) -> dict:
        """
        Monta um prompt contextualizado b√°sico para fallback ou integra√ß√£o com outros modelos.
        Retorna dicion√°rio compat√≠vel com uso esperado (mensagens, max_tokens, temperature).
        """
        prompt = f"Usu√°rio: {context.user_name or 'Desconhecido'}\n"
        prompt += f"Mensagem: {context.user_message}\n"
        prompt += f"N√≠vel de risco: {context.risk_level.value}\n"
        if context.emotional_state:
            prompt += f"Estado emocional: {context.emotional_state}\n"
        if context.dominant_themes:
            prompt += f"Temas principais: {', '.join(context.dominant_themes)}\n"
        prompt += "Responda de forma emp√°tica, breve e √∫til."

        # Estrutura compat√≠vel com OpenAI/Gemini
        messages = [
            {"role": "system", "content": "Voc√™ √© um agente de suporte emocional. Responda de forma emp√°tica, breve e √∫til."},
            {"role": "user", "content": prompt}
        ]
        return {
            "messages": messages,
            "prompt": prompt,
            "max_tokens": 200,
            "temperature": 0.7
        }
    
    # Configura√ß√µes de provider podem ser movidas para um arquivo utilit√°rio se necess√°rio
    
    # Fun√ß√£o de an√°lise de humor pode ser movida para utilit√°rio se desejado
    
    # Fun√ß√£o de adapta√ß√£o pode ser movida para utilit√°rio se desejado
    
    # Valida√ß√£o de tamanho de prompt pode ser utilit√°rio
    
    # Inicializa√ß√£o de templates, estrat√©gias e adapta√ß√µes removida. Toda l√≥gica de prompt est√° nos arquivos prompts_openai.py e prompts_gemini.py
    
    def _determine_therapeutic_approach(self, context: PromptContext) -> PromptType:
        """Determina a melhor abordagem terap√™utica baseada no contexto"""
        
        # Casos cr√≠ticos sempre usam interven√ß√£o de crise
        if context.risk_level == RiskLevel.CRITICAL:
            return PromptType.CRISIS_INTERVENTION
        
        # Se h√° abordagem espec√≠fica solicitada
        if context.therapeutic_approach:
            return context.therapeutic_approach
        
        # Baseado no conte√∫do da mensagem
        message_lower = context.user_message.lower()
        
        # Indicadores para diferentes abordagens
        cognitive_indicators = ['penso que', 'acredito que', 'tenho certeza', 'sempre acontece', 'nunca consigo']
        mindfulness_indicators = ['ansioso', 'acelerado', 'mente correndo', 'n√£o paro de pensar']
        solution_indicators = ['o que fazer', 'como resolver', 'preciso de ajuda para', 'n√£o sei como']
        
        # Contar indicadores
        cognitive_score = sum(1 for indicator in cognitive_indicators if indicator in message_lower)
        mindfulness_score = sum(1 for indicator in mindfulness_indicators if indicator in message_lower)
        solution_score = sum(1 for indicator in solution_indicators if indicator in message_lower)
        
        # Determinar abordagem
        if solution_score >= 2:
            return PromptType.SOLUTION_FOCUSED
        elif cognitive_score >= 2:
            return PromptType.COGNITIVE_BEHAVIORAL
        elif mindfulness_score >= 2:
            return PromptType.MINDFULNESS_BASED
        elif context.risk_level == RiskLevel.HIGH:
            return PromptType.CRISIS_INTERVENTION
        else:
            return PromptType.EMPATHETIC_RESPONSE
    # (Bloco de fallback removido ap√≥s unifica√ß√£o do prompt)
    
    def get_prompt_statistics(self) -> Dict:
        """Retorna estat√≠sticas sobre uso de prompts"""
        return {
            'available_prompt_types': [pt.value for pt in PromptType],
            'risk_levels': [rl.value for rl in RiskLevel],
            'templates_count': len(self.prompt_templates),
            'strategies_count': len(self.therapeutic_strategies),
            'risk_adaptations_count': len(self.risk_adaptations)
        }
    
    def validate_prompt_context(self, context: PromptContext) -> Tuple[bool, List[str]]:
        """Valida se o contexto do prompt est√° completo"""
        
        errors = []
        
        if not context.user_message or not context.user_message.strip():
            errors.append("Mensagem do usu√°rio n√£o pode estar vazia")
        
        if not isinstance(context.risk_level, RiskLevel):
            errors.append("N√≠vel de risco deve ser um RiskLevel v√°lido")
        
        if context.session_history and not isinstance(context.session_history, list):
            errors.append("Hist√≥rico da sess√£o deve ser uma lista")
        
        if context.dominant_themes and not isinstance(context.dominant_themes, list):
            errors.append("Temas dominantes devem ser uma lista")
        
        return len(errors) == 0, errors


# --- Fun√ß√µes de conveni√™ncia ---
def create_prompt_manager() -> AIPromptManager:
    """Cria inst√¢ncia configurada do AIPromptManager."""
    return AIPromptManager()

# Inst√¢ncia global para uso direto
prompt_manager = AIPromptManager()
